{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amansyayf/Dreambooth_LoRA/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning with LoRA\n",
        "\n",
        "A notebook for training Stable Diffusion models using Low-rank Adaptation (LoRA) approaches.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EDwM5xLRggN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies (takes about 1 minute)"
      ],
      "metadata": {
        "id": "LouTFfVYhRei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd /content/\n",
        "!git clone https://github.com/amansyayf/Dreambooth_LoRA\n",
        "!pip install -r \"Dreambooth_LoRA/requirements.txt\"\n",
        "!pip install -U --pre triton\n",
        "!pip install torchinfo\n",
        "\n",
        "!git clone https://github.com/brian6091/lora --branch v0.0.5 --single-branch\n",
        "!python -m pip install /content/lora/"
      ],
      "metadata": {
        "id": "PmsR_IPcvp7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title xformers\n",
        "#%%capture\n",
        "\n",
        "!nvidia-smi -L\n",
        "!pip install xformers==0.0.16rc425"
      ],
      "metadata": {
        "id": "qf74-EgsepNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model to train"
      ],
      "metadata": {
        "id": "8C3LrrpBvYn-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI6dHfQ8iqMg"
      },
      "outputs": [],
      "source": [
        "#@title ## Name or path to initial model\n",
        "#@markdown Obligatory (e.g., runwayml/stable-diffusion-v1-5, stabilityai/stable-diffusion-2-base, or full path to model in diffusers format)\n",
        "MODEL_NAME_OR_PATH = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "G9Yj5NZ6er_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up experiment parameters"
      ],
      "metadata": {
        "id": "fxP_d_n4mW2_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1cJJ-P8jPhx"
      },
      "outputs": [],
      "source": [
        "#@title ## Training parameters\n",
        "\n",
        "import os\n",
        "from IPython.display import Markdown as md\n",
        "\n",
        "#@markdown Unique token for specific subject\n",
        "INSTANCE_TOKEN= \"sks\" #@param{type: 'string'}\n",
        "\n",
        "#@markdown Use image captions? Captions can be either the image filename, or a separate text file (that must be named identically to the image but w/ extension .txt). If a separate .txt file exists, filename is ignored.\n",
        "USE_IMAGE_CAPTIONS = False #@param {type:\"boolean\"}\n",
        "USE_IMAGE_CAPTIONS_FLAG = \"\"\n",
        "if USE_IMAGE_CAPTIONS:\n",
        "  USE_IMAGE_CAPTIONS_FLAG='--use_image_captions'\n",
        "\n",
        "#@markdown Path to instance images. Filenames are irrelevant, unless images are captioned *and* captions are not separate textfiles, in which case INSTANCE_TOKEN should appear in relevant filenames as part of the caption.\n",
        "INSTANCE_DIR=\"/content/gdrive/MyDrive/InstanceImages\" #@param{type: 'string'}\n",
        "\n",
        "RESOLUTION = 512 #@param{type: 'number'}\n",
        "\n",
        "TRAIN_BATCH_SIZE = 1 #@param{type: 'number'}\n",
        "\n",
        "GRADIENT_ACCUMULATION_STEPS = 1  #@param{type: 'number'}\n",
        "\n",
        "GRADIENT_CHECKPOINTING = True #@param {type:\"boolean\"}\n",
        "GRADIENT_CHECKPOINTING_FLAG=\"\"\n",
        "if GRADIENT_CHECKPOINTING:\n",
        "  GRADIENT_CHECKPOINTING_FLAG='--gradient_checkpointing'\n",
        "\n",
        "ENABLE_PRIOR_PRESERVATION = True #@param {type:\"boolean\"}\n",
        "ENABLE_PRIOR_PRESERVATION_FLAG=\"\"\n",
        "if ENABLE_PRIOR_PRESERVATION:\n",
        "  ENABLE_PRIOR_PRESERVATION_FLAG='--with_prior_preservation'\n",
        "\n",
        "#@markdown Prior loss weight. Note that if you set this to 0, but enable prior preservation and provide a CLASS_DIR, you can still monitor class loss.\n",
        "PRIOR_LOSS_WEIGHT = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown If using prior preservation, specify a path to class images\n",
        "CLASS_DIR=\"/content/gdrive/MyDrive/RegularizationImages\" #@param{type: 'string'}\n",
        "if (CLASS_DIR !=\"\") and os.path.exists(str(CLASS_DIR)):\n",
        "  CLASS_DIR=CLASS_DIR\n",
        "elif (CLASS_DIR !=\"\") and not os.path.exists(str(CLASS_DIR)):\n",
        "  CLASS_DIR=input('\u001b[1;31mThe folder specified does not exist, use the colab file explorer to copy the path :')\n",
        "\n",
        "#@markdown Prompt for prior preservation class (e.g., 'person', 'a photo of a man', 'dog'). Ignored if USE_IMAGE_CAPTIONS checked.\n",
        "CLASS_PROMPT=\"penguin\" #@param {type:\"string\"}\n",
        "#@markdown Instance prompt, {SKS} will be automatically replaced by INSTANCE_TOKEN defined above.  Ignored if USE_IMAGE_CAPTIONS checked.\n",
        "INSTANCE_PROMPT=\"{SKS} penguin\" #@param {type:\"string\"}\n",
        "INSTANCE_PROMPT=INSTANCE_PROMPT.replace(\"{SKS}\",INSTANCE_TOKEN)\n",
        "\n",
        "#@markdown Specify the number of class images used if prior preservation is enabled. If there are not enough images in CLASS_DIR (or CLASS_DIR is empty), additional images will be generated.\n",
        "MIN_NUM_CLASS_IMAGES=100 #@param{type: 'number'}\n",
        "\n",
        "#@markdown Batch size for generating class images\n",
        "SAMPLE_BATCH_SIZE = 1 #@param{type: 'number'}\n",
        "\n",
        "#@markdown Number of training iterations, e.g., # instance images * 100\n",
        "STEPS = 2500 #@param{type: 'number'}\n",
        "\n",
        "#@markdown Random number generator seed\n",
        "SEED = 2000000 #@param{type: 'number'}\n",
        "\n",
        "#@markdown Enable text encoder training?\n",
        "TRAIN_TEXT_ENCODER = True #@param{type: 'boolean'}\n",
        "TRAIN_TEXT_ENCODER_FLAG=\"\"\n",
        "if TRAIN_TEXT_ENCODER:\n",
        "  TRAIN_TEXT_ENCODER_FLAG=\"--train_text_encoder\"\n",
        "\n",
        "#@markdown ## ADAM optimizer settings\n",
        "\n",
        "\n",
        "\n",
        "#@markdown The exponential decay rate for the 1st moment estimates (the beta1 parameter for the Adam optimizer).\n",
        "ADAM_BETA1 = 0.9 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown The exponential decay rate for the 2nd moment estimates (the beta2 parameter for the Adam optimizer).\n",
        "ADAM_BETA2 = 0.999 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Weight decay magnitude for the Adam optimizer.\n",
        "ADAM_WEIGHT_DECAY = 1e-2 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Epsilon value for the Adam optimizer.\n",
        "ADAM_EPSILON = 1e-08 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown \"fp16\", \"bf16\", or \"no\" according to available VRAM\n",
        "MIXED_PRECISION = \"fp16\" #@param{type: 'string'}\n",
        "\n",
        "#@markdown ## Learning rate parameters\n",
        "LR_SCHEDULE = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
        "LR = 1e-4 #@param{type: 'number'}\n",
        "#@markdown If training the text encoder, a different learning rate can be applied\n",
        "LR_TEXT_ENCODER = 5e-5 #@param{type: 'number'}\n",
        "LR_WARMUP_STEPS = 50 #@param{type: 'number'}\n",
        "#@markdown Applies only for cosine_with_restarts schedule\n",
        "LR_COSINE_NUM_CYCLES = 5 #@param{type: 'number'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title # (Experimental) [Data augmentation](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0/)\n",
        "# #@markdown Transformations to apply to images (both instance and class).\n",
        "# #@markdown I find this useful to minimize the work of cropping and manually preparing images.\n",
        "# #@markdown This may be useful for certain applications, such as training a style, where there may not be a specific subject in each image.\n",
        "# #@markdown In this case, I don't crop images, and I enable random cropping, which presents to the network a randomly cropped (RESOLUTION X RESOLUTION) chunk of the original image selected for that iteration.\n",
        "# #@markdown AUGMENT_MIN_RESOLUTION allows you to adjust how much of the image you will crop. So if you are training for RESOLUTION=512, setting AUGMENT_MIN_RESOLUTION will give you two crops (on average) for the shortest image dimension.\n",
        "\n",
        "\n",
        "\n",
        "#@markdown If not enabled, defaults to center crop (which will do nothing if your images are already square at the RESOLUTION set above).\n",
        "AUGMENT_RANDOM_CROP = False #@param{type: 'boolean'}\n",
        "AUGMENT_CENTER_CROP_FLAG=\"--augment_center_crop\"\n",
        "if AUGMENT_RANDOM_CROP:\n",
        "  AUGMENT_CENTER_CROP_FLAG=\"\"\n",
        "\n",
        "#@markdown Randomly flip image horizontally. Not recommended if asymmetry is important (e.g., faces).\n",
        "AUGMENT_HFLIP = False #@param{type: 'boolean'}\n",
        "AUGMENT_HFLIP_FLAG=\"\"\n",
        "if AUGMENT_HFLIP:\n",
        "  AUGMENT_HFLIP_FLAG=\"--augment_hflip\""
      ],
      "metadata": {
        "id": "zLGpiF7xsLcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title # (Experimental) other training parameters\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Rank of LoRA update matrix\n",
        "LORA_RANK = 4 #@param{type: 'number'}\n",
        "\n",
        "\n",
        "#@markdown ## Exponentially-weight moving average weights (unet only). Will not run on Tesla T4 (out of memory).\n",
        "USE_EMA = False #@param{type: 'boolean'}\n",
        "USE_EMA_FLAG=\"\"\n",
        "if USE_EMA:\n",
        "  USE_EMA_FLAG=\"--use_ema\"\n",
        "EMA_INV_GAMMA = 1.0 #@param{type: 'number'}\n",
        "EMA_POWER = 0.75 #@param{type: 'number'}\n",
        "EMA_MIN_VALUE = 0 #@param{type: 'number'}\n",
        "EMA_MAX_VALUE = 0.9999 #@param{type: 'number'}"
      ],
      "metadata": {
        "id": "LU7NC1Pkr47k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Where should outputs get saved?\n",
        "\n",
        "#@markdown Trained models (and intermediates) saved here\n",
        "OUTPUT_DIR=\"/content/gdrive/MyDrive/experiment\" #@param{type: 'string'}\n",
        "\n",
        "#@markdown Training logs saved here\n",
        "LOGGING_DIR=\"/content/logs/\" #@param{type: 'string'}\n",
        "\n",
        "if not os.path.exists(LOGGING_DIR):\n",
        "  !mkdir -p \"$LOGGING_DIR\"\n",
        "\n",
        "LOG_GPU = True #@param{type: 'boolean'}\n",
        "if LOG_GPU:\n",
        "  LOG_GPU_FLAG=\"--log_gpu\"\n",
        "else:\n",
        "  LOG_GPU_FLAG=\"\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Lji3GATOYIg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Setup saving of intermediate models\n",
        "#@markdown To save intermediate checkpoints, set START_SAVING_FROM_STEP < STEPS\n",
        "\n",
        "#@markdown Number of steps between intermediate saves\n",
        "SAVE_CHECKPOINT_EVERY = 500 #@param{type: 'number'}\n",
        "if SAVE_CHECKPOINT_EVERY==None:\n",
        "  SAVE_CHECKPOINT_EVERY = STEPS+1\n",
        "\n",
        "START_SAVING_FROM_STEP=500 #@param{type: 'number'}\n",
        "if START_SAVING_FROM_STEP==None:\n",
        "  START_SAVING_FROM_STEP=STEPS\n",
        "\n",
        "#@markdown At each intermediate checkpoint, infer this many samples using SAVE_SAMPLE_PROMPT\n",
        "N_SAVE_SAMPLES=3 #@param{type: 'number'}\n",
        "\n",
        "#@markdown {SKS} is automatically replaced by INSTANCE_TOKEN. Give multiple prompts using // as a separator\n",
        "SAVE_SAMPLE_PROMPT= \"{SKS} penguin // close-up {SKS} penguin// {SKS} penguin riding a bicycle\" #@param{type: 'string'}\n",
        "if SAVE_SAMPLE_PROMPT==\"\":\n",
        "  SAVE_SAMPLE_PROMPT=None\n",
        "else:\n",
        "  SAVE_SAMPLE_PROMPT=SAVE_SAMPLE_PROMPT.replace(\"{SKS}\",INSTANCE_TOKEN)\n",
        "\n",
        "#@markdown The negative prompt, on the other hand, applies to all SAVE_SAMPLE_PROMPTs\n",
        "SAVE_SAMPLE_NEGATIVE_PROMPT=\"\" #@param{type: 'string'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m9wXEuCnXn_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train!"
      ],
      "metadata": {
        "id": "-sWFt9CCYkMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Launch training\n",
        "!lsb_release -a | grep Description\n",
        "!pip freeze | grep diffusers\n",
        "!pip freeze | grep lora-diffusion\n",
        "!pip freeze | grep torchvision\n",
        "!pip freeze | grep transformers\n",
        "!pip freeze | grep xformers\n",
        "!accelerate env\n",
        "\n",
        "!accelerate launch \\\n",
        "    --mixed_precision=$MIXED_PRECISION \\\n",
        "    --num_machines=1 \\\n",
        "    --num_processes=1 \\\n",
        "    --dynamo_backend=\"no\" \\\n",
        "    /content/Dreambooth_LoRA/train.py \\\n",
        "    --lora_rank=$LORA_RANK \\\n",
        "    $TRAIN_TEXT_ENCODER_FLAG \\\n",
        "    --pretrained_model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
        "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
        "    --class_data_dir=\"$CLASS_DIR\" \\\n",
        "    --output_dir=\"$OUTPUT_DIR\" \\\n",
        "    --logging_dir=\"$LOGGING_DIR\" \\\n",
        "    $LOG_GPU_FLAG \\\n",
        "    $ENABLE_PRIOR_PRESERVATION_FLAG \\\n",
        "    --prior_loss_weight=$PRIOR_LOSS_WEIGHT \\\n",
        "    --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
        "    --class_prompt=\"$CLASS_PROMPT\" \\\n",
        "    --seed=$SEED \\\n",
        "    --resolution=$RESOLUTION \\\n",
        "    --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
        "    --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
        "    $GRADIENT_CHECKPOINTING_FLAG \\\n",
        "    --mixed_precision=$MIXED_PRECISION \\\n",
        "    --use_8bit_adam \\\n",
        "    --adam_beta1=0.9 \\\n",
        "    --adam_beta2=0.999 \\\n",
        "    --adam_weight_decay=0.01 \\\n",
        "    --adam_epsilon=0.00000001 \\\n",
        "    --learning_rate=$LR \\\n",
        "    --learning_rate_text=$LR_TEXT_ENCODER \\\n",
        "    --lr_scheduler=$LR_SCHEDULE \\\n",
        "    --lr_warmup_steps=$LR_WARMUP_STEPS \\\n",
        "    --lr_cosine_num_cycles=$LR_COSINE_NUM_CYCLES \\\n",
        "    $USE_EMA_FLAG \\\n",
        "    --ema_inv_gamma=$EMA_INV_GAMMA \\\n",
        "    --ema_power=$EMA_POWER \\\n",
        "    --ema_min_value=$EMA_MIN_VALUE \\\n",
        "    --ema_max_value=$EMA_MAX_VALUE \\\n",
        "    --max_train_steps=$STEPS \\\n",
        "    --num_class_images=$MIN_NUM_CLASS_IMAGES \\\n",
        "    --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
        "    --save_min_steps=$START_SAVING_FROM_STEP \\\n",
        "    --save_interval=$SAVE_CHECKPOINT_EVERY \\\n",
        "    --n_save_sample=$N_SAVE_SAMPLES \\\n",
        "    --save_sample_prompt=\"$SAVE_SAMPLE_PROMPT\" \\\n",
        "    --save_sample_negative_prompt=\"$SAVE_SAMPLE_NEGATIVE_PROMPT\" \\\n",
        "    $AUGMENT_CENTER_CROP_FLAG \\\n",
        "    $AUGMENT_HFLIP_FLAG"
      ],
      "metadata": {
        "id": "2ENlDgjlezXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Close Colab instance"
      ],
      "metadata": {
        "id": "_fE3xIr7lE2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "HYrrT17slGd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}