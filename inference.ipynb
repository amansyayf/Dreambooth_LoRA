{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amansyayf/Dreambooth_LoRA/blob/main/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt8Oi12oTPKa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!cd /content/\n",
        "!git clone https://github.com/amansyayf/Dreambooth_LoRA\n",
        "!pip install -r \"Dreambooth_LoRA/requirements.txt\"\n",
        "!pip install -U --pre triton\n",
        "!pip install torchinfo\n",
        "\n",
        "!git clone https://github.com/brian6091/lora --branch v0.0.5 --single-branch\n",
        "!python -m pip install /content/lora/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L\n",
        "\n",
        "# Tested with Tesla T4 and A100 GPUs\n",
        "!pip install xformers==0.0.16rc425"
      ],
      "metadata": {
        "id": "kb7if7FQc-u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "dJnTq6OydBFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhOOEEBaDI1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline, StableDiffusionPipeline, DPMSolverMultistepScheduler, AutoencoderKL\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "from lora_diffusion import monkeypatch_lora, tune_lora_scale\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    \"\"\"\n",
        "        Makes grid of given images.\n",
        "    \"\"\"\n",
        "    assert len(imgs) == rows*cols\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def get_pipeline(model_name_or_path,\n",
        "                 vae_name_or_path=None,\n",
        "                 text_encoder_name_or_path=None,\n",
        "                 feature_extractor_name_or_path=None,\n",
        "                 revision=\"fp16\"):\n",
        "    \"\"\"\n",
        "        Loades pipeline of untrained original model.\n",
        "    \"\"\"\n",
        "    scheduler = DPMSolverMultistepScheduler(\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        num_train_timesteps=1000,\n",
        "        trained_betas=None,\n",
        "        prediction_type=\"epsilon\",\n",
        "        thresholding=False,\n",
        "        algorithm_type=\"dpmsolver++\",\n",
        "        solver_type=\"midpoint\",\n",
        "        lower_order_final=True,\n",
        "    )\n",
        "\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        safety_checker=None,\n",
        "        revision=revision,\n",
        "        scheduler=scheduler,\n",
        "        vae=AutoencoderKL.from_pretrained(\n",
        "            vae_name_or_path or model_name_or_path,\n",
        "            subfolder=None if vae_name_or_path else \"vae\",\n",
        "            revision=None if vae_name_or_path else revision,\n",
        "            torch_dtype=torch.float16,\n",
        "        ),\n",
        "        feature_extractor=feature_extractor_name_or_path,\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    #https://github.com/huggingface/diffusers/issues/1552\n",
        "    #pipe.enable_attention_slicing()\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "    return pipe\n",
        "\n",
        "# Monkey patch LoRA pt files\n",
        "# Returns pipeline\n",
        "def get_lora_pipeline(model_dir, scale_unet=1.0, scale_text_encoder=1.0):\n",
        "    \"\"\"\n",
        "        Makes grid of inference images.\n",
        "    \"\"\"\n",
        "\n",
        "    pipe = get_pipeline(MODEL_NAME_OR_PATH)\n",
        "\n",
        "    print('Monkey patching unet pt file')\n",
        "    monkeypatch_lora(pipe.unet, torch.load(os.path.join(model_dir, \"lora_unet.pt\")))\n",
        "\n",
        "    print('Monkey patching text encoder pt file')\n",
        "    monkeypatch_lora(pipe.text_encoder, torch.load(os.path.join(model_dir, \"lora_text_encoder.pt\")), target_replace_module=[\"CLIPAttention\"])\n",
        "\n",
        "    tune_lora_scale(pipe.unet, scale_unet)\n",
        "    tune_lora_scale(pipe.text_encoder, scale_text_encoder)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def get_config(filename=None,\n",
        "               save_dir=None,\n",
        "               prompt=None, negative_prompt=None,\n",
        "               seeds=None,\n",
        "               num_samples=4,\n",
        "               width=512, height=512,\n",
        "               inference_steps=20,\n",
        "               guidance_scale=7.5,\n",
        "               ):\n",
        "    \"\"\"\n",
        "        Creates needed configuration for loading inference grid\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if filename==None:\n",
        "        num_prompts = len(prompt)\n",
        "        if seeds==None:\n",
        "            seeds = []\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                seeds.append(i * 1000000)\n",
        "        else:\n",
        "            num_samples = len(seeds)\n",
        "\n",
        "        tag = ''.join(random.choice(string.ascii_letters) for _ in range(8))\n",
        "        config = {\n",
        "            \"tag\": tag,\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"num_prompts\": num_prompts,\n",
        "            \"num_samples\": num_samples,\n",
        "            \"seeds\": seeds,\n",
        "            \"height\": height,\n",
        "            \"width\": width,\n",
        "            \"inference_steps\": inference_steps,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(save_dir, \"config_\"+tag+\".json\"), \"w\") as outfile:\n",
        "            json.dump(config, outfile)\n",
        "    else:\n",
        "        f = open(filename)\n",
        "        config = json.load(f)\n",
        "\n",
        "    return config\n",
        "\n",
        "def get_images(pipe, sample_config, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "        Creates inference images\n",
        "\n",
        "    \"\"\"\n",
        "    generator = torch.Generator(\"cuda\")\n",
        "    with torch.autocast(device):\n",
        "        num_cfg = len(sample_config['guidance_scale'])\n",
        "        # Loop in order to use defined seed for each image in a batch\n",
        "        all_images = []\n",
        "        for i in range(sample_config['num_samples']):\n",
        "            for cfg in sample_config['guidance_scale']:\n",
        "                # Manually generate latent\n",
        "                seed = sample_config['seeds'][i]\n",
        "                generator = generator.manual_seed(seed)\n",
        "                latent = torch.randn(\n",
        "                    (1, pipe.unet.in_channels, sample_config['height'] // 8, sample_config['width'] // 8),\n",
        "                    generator = generator,\n",
        "                    device = device\n",
        "                )\n",
        "                images = pipe(sample_config['prompt'],\n",
        "                    negative_prompt=sample_config['negative_prompt'] * len(sample_config['prompt']),\n",
        "                    num_inference_steps=int(sample_config['inference_steps']),\n",
        "                    guidance_scale=cfg,\n",
        "                    latents=latent.repeat(sample_config['num_prompts'], 1, 1, 1),\n",
        "                ).images\n",
        "                all_images.extend(images)\n",
        "\n",
        "    return all_images\n",
        "\n",
        "def make_reversed_order(images, rows, cols):\n",
        "    \"\"\"\n",
        "        Changes order of images for grid image\n",
        "\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    for i in range(rows):\n",
        "      for j in range(cols):\n",
        "        images.append(all_images[rows*j+i])\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i0wF1LdCWed"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME_OR_PATH = \"runwayml/stable-diffusion-v1-5\"\n",
        "OUTPUT_DIR =  \"/content/gdrive/MyDrive/experiment\" #Where trained LoRA models are located"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Specify which models to do inference with\n",
        "model_list = [\n",
        "              os.path.join(OUTPUT_DIR,'500'),\n",
        "              os.path.join(OUTPUT_DIR,'1000'),\n",
        "              os.path.join(OUTPUT_DIR,'1500'),\n",
        "              os.path.join(OUTPUT_DIR,'2000'),\n",
        "              os.path.join(OUTPUT_DIR,'2500'),\n",
        "              ]\n",
        "\n",
        "print(model_list)"
      ],
      "metadata": {
        "id": "FFaT5VjBdP6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate or load a configuration for inference\n",
        "\n",
        "config_name = None\n",
        "#config_name = os.path.join(OUTPUT_DIR, \"config_ZMasiqkP.json\")\n",
        "\n",
        "if config_name is None:\n",
        "\n",
        "    prompt = [\"sks penguin\", \"close-up sks penguin\", \"sks penguin in front of eiffel tower\", \"sks penguin riding a bicycle\", \"sks penguin wearing sunglasses and holding a phone\"]\n",
        "    negative_prompt = [\"\"]\n",
        "    guidance_scale = [7.5]\n",
        "    seeds = [2000000]\n",
        "\n",
        "\n",
        "    config = get_config(save_dir=OUTPUT_DIR,\n",
        "                        prompt=prompt, negative_prompt=negative_prompt,\n",
        "                        seeds = seeds,\n",
        "                        width=512, height=512,\n",
        "                        inference_steps=50, guidance_scale=guidance_scale\n",
        "                        )\n",
        "else:\n",
        "    config = get_config(filename=config_name)\n"
      ],
      "metadata": {
        "id": "m8A_Bg8ad2xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make inference\n",
        "\n",
        "LORA_SCALE_UNET = 1.0\n",
        "LORA_SCALE_TENC = 1.0\n",
        "\n",
        "all_images = []\n",
        "\n",
        "for model in model_list:\n",
        "    pipe = get_lora_pipeline(model, scale_unet=LORA_SCALE_UNET, scale_text_encoder=LORA_SCALE_TENC)\n",
        "    images = get_images(pipe, config)\n",
        "    display(images)\n",
        "    all_images.extend(images)\n",
        "\n",
        "    del pipe\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zeYAWY1Rd5es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = make_reversed_order(all_images, 5, 5)\n",
        "grid = image_grid(images, rows=5, cols=5)\n",
        "grid.save(os.path.join(OUTPUT_DIR, \"grid.jpg\"), quality=90, optimize=True)"
      ],
      "metadata": {
        "id": "qlk3iMfgYyIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv5ei8giYhiv"
      },
      "outputs": [],
      "source": [
        "#@title # Close Colab instance\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}